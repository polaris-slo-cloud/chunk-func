{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0e7ce5-f7d0-4f1e-b917-deaf3466fa73",
   "metadata": {},
   "source": [
    "# Profiling Results Accuracy\n",
    "\n",
    "This file compares the accuracy of the following profiling results:\n",
    "\n",
    "* Baseline: exhaustive profiling (159 resource profiles, based on the complete AWS profiles range in 64 MB steps)\n",
    "* Linear model (linear interpolation between an evenly distributed selection of 5% of the exhaustive profiling results)\n",
    "* Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64b2e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Workaround for using the Jupyter container from VS Code Jupyter extension.\n",
    "if not os.path.exists('./profiling-results'):\n",
    "    os.chdir('./chunk-func/jupyter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2591b972-c9fe-4c9b-afd9-4182b091a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Definitions of our column names.\n",
    "col_profile = 'profile'\n",
    "col_exec_time = 'execTime'\n",
    "col_cost = 'cost'\n",
    "col_profiled = 'profiled'\n",
    "\n",
    "@dataclass\n",
    "class FunctionProfilingResults:\n",
    "    input_sizes: list[int]\n",
    "    '''All input sizes for this function in ascending order.'''\n",
    "    \n",
    "    results: dict[int, pd.DataFrame]\n",
    "    '''The results indexed by input size.'''\n",
    "\n",
    "\n",
    "def extract_input_sizes(func_desc) -> list[int]:\n",
    "    ret: list[int] = []\n",
    "    typical_inputs = func_desc['spec']['functionDescription']['typicalInputs']\n",
    "    for input in typical_inputs:\n",
    "        ret.append(input['sizeBytes'])\n",
    "    return ret\n",
    "\n",
    "def add_result(profile_result, dest_results_per_input: dict[int, list[dict[str, object]]]):\n",
    "    profile = profile_result['resourceProfileId']\n",
    "    results = profile_result.get('results')\n",
    "    \n",
    "    if results is not None:\n",
    "        for input_result in results:\n",
    "            results_list = dest_results_per_input[input_result['inputSizeBytes']]\n",
    "            results_list.append({ \n",
    "                col_profile: profile, \n",
    "                col_exec_time: input_result['executionTimeMs'], \n",
    "                col_cost: float(input_result['executionCost']),\n",
    "                col_profiled: input_result.get('resultType', 'Profiled') == 'Profiled',\n",
    "            })\n",
    "\n",
    "\n",
    "def load_result(path: str) -> FunctionProfilingResults:\n",
    "    '''\n",
    "    Reads a FunctionDescription YAML file and returns a dictionary of the profiling results,\n",
    "    indexed by input size.\n",
    "    '''\n",
    "    with open(path, 'r') as file:\n",
    "        func_desc = yaml.safe_load(file)\n",
    "    input_sizes = extract_input_sizes(func_desc)\n",
    "    results_per_input = { \n",
    "        input_size: [] for input_size in input_sizes \n",
    "    }\n",
    "\n",
    "    all_results = func_desc['status']['profilingResults']['results']\n",
    "    for profile_result in all_results:\n",
    "        add_result(profile_result, results_per_input)\n",
    "\n",
    "    data_frames: dict[int, pd.DataFrame] = {}\n",
    "    for input_size, results in results_per_input.items():\n",
    "        df = pd.DataFrame(data=results, columns=[col_profile, col_exec_time, col_cost, col_profiled])\n",
    "        df = df.set_index(col_profile)\n",
    "        data_frames[input_size] = df\n",
    "    return FunctionProfilingResults(input_sizes=input_sizes, results=data_frames)\n",
    "\n",
    "\n",
    "def load_results(functions: list[str], profiling_type: str) -> dict[str, FunctionProfilingResults]:\n",
    "    '''\n",
    "    Loads the results for all specified functions. profiling_type may be 'aws', 'gcf', or 'bo'.\n",
    "    '''\n",
    "    ret: dict[str, dict[int, pd.DataFrame]] = {\n",
    "        func: load_result(f'./profiling-results/{profiling_type}/{func}.yaml') for func in functions\n",
    "    }\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97840053-429b-43cb-823e-63cc6bcb99da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_functions = [\n",
    "    # LogPro\n",
    "    'validate-log',\n",
    "    'extract-basic-stats',\n",
    "    'extract-successes',\n",
    "    'extract-success-stats',\n",
    "\n",
    "    # VidPro\n",
    "    'validate-video',   \n",
    "    'cut-video',\n",
    "    'merge-videos',\n",
    "\n",
    "    # FaceDet\n",
    "    'validate-video-face-recog',\n",
    "    'transform-video',\n",
    "    'detect-faces',\n",
    "    'mark-faces',\n",
    "]\n",
    "\n",
    "aws_results = load_results(all_functions, 'aws')\n",
    "exhaustive_results = aws_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "337ec354-3023-490c-afa7-fa04b517deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_xi='0.01'\n",
    "bo_results = load_results(all_functions, f'bo/poi=0.02-minSamples=0.1/xi={bo_xi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fc5645e-1f7d-4c81-a83b-b4d9b0ce756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def matplot_exhaustive_bo_comparison(exhaustive_result: pd.DataFrame, bo_result: pd.DataFrame):\n",
    "    fig, ax = plt.subplots()\n",
    "    exhaustive_result[[col_exec_time]].plot(ax=ax, label='Exhaustive')\n",
    "    bo_result[[col_exec_time]].plot(ax=ax, label='BO')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97c68142-3744-42a4-8867-1daf7ba1478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "def seaplot_exhaustive_bo_comparison(exhaustive_result: pd.DataFrame, bo_result: pd.DataFrame, title: str):\n",
    "    exhaustive_result = exhaustive_result[[col_exec_time]].rename(columns={col_exec_time: 'Exhaustive'})\n",
    "    bo_result_renamed = bo_result[[col_exec_time]].rename(columns={col_exec_time: 'BO'})\n",
    "    joined = exhaustive_result.join(bo_result_renamed)\n",
    "    melted = joined.reset_index().melt(id_vars=[col_profile], var_name='type', value_name=col_exec_time)\n",
    "    g: sns.FacetGrid = sns.relplot(\n",
    "        data=melted,\n",
    "        kind='line',\n",
    "        x=col_profile,\n",
    "        y=col_exec_time,\n",
    "        hue='type',\n",
    "        facet_kws=dict(sharex=True),\n",
    "    )\n",
    "    g.set_axis_labels('Profile', 'Execution Time (ms)')\n",
    "    g.set_xticklabels(step=10, rotation=45)\n",
    "    g.ax.set_title(title)\n",
    "\n",
    "    # Mark the samples that BO decided to profile (the others were inferred)\n",
    "    bo_profiled = bo_result[bo_result[col_profiled] == True].copy()\n",
    "    bo_profiled.loc[:, 'type'] = 'BO'\n",
    "    sns.scatterplot(\n",
    "        ax=g.ax,\n",
    "        data=bo_profiled,\n",
    "        x=col_profile,\n",
    "        y=col_exec_time,\n",
    "        hue='type',\n",
    "        style='type',\n",
    "        legend=False,\n",
    "        palette=[(0, 0, 0)],  #[sns.color_palette('flare')[0]],\n",
    "        markers=['X'],\n",
    "        zorder=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d8c7f8b-b335-41d3-9ba3-3b6dcb8e65f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def normalized_rmse(exhaustive_result: pd.DataFrame, predicted_result: pd.DataFrame) -> float:\n",
    "    '''Computes the normalized RMSE for a single input size.'''\n",
    "    y_exhaustive_df: pd.DataFrame = exhaustive_result[[col_exec_time]]\n",
    "    y_pred_df: pd.DataFrame = predicted_result[[col_exec_time]]\n",
    "\n",
    "    # Due to errors for certain profiles in the exhaustive runs, the two DataFrames might not contain exactly the same profiles,\n",
    "    # e.g., the profiling for 256mib went into timeout, while the predicted results inferred a value for this profile.\n",
    "    # We want to compare only those profiles which are present in both DataFrames.\n",
    "    merged = y_exhaustive_df.merge(y_pred_df, how='inner', on=[col_profile], suffixes=('_exhaustive', '_predicted'))\n",
    "    y_exhaustive = merged[f'{col_exec_time}_exhaustive']\n",
    "    y_pred = merged[f'{col_exec_time}_predicted']\n",
    "\n",
    "    rmse = root_mean_squared_error(y_exhaustive, y_pred)\n",
    "    y_ex_mean = y_exhaustive.mean()\n",
    "    return rmse / y_ex_mean\n",
    "\n",
    "\n",
    "def mean_normalized_rmse(exhaustive_results: FunctionProfilingResults, predicted_results: FunctionProfilingResults) -> float:\n",
    "    '''Computes the normalized RMSE for all input sizes and returns the mean average.'''\n",
    "    rmse_sum = 0.0\n",
    "    for input_size in exhaustive_results.input_sizes:\n",
    "        exhaustive = exhaustive_results.results[input_size]\n",
    "        predicted = predicted_results.results[input_size]\n",
    "        rmse = normalized_rmse(exhaustive_result=exhaustive, predicted_result=predicted)\n",
    "        rmse_sum += rmse\n",
    "    \n",
    "    mean_rmse = rmse_sum / float(len(exhaustive_results.input_sizes))\n",
    "    return mean_rmse\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a7d999a-5ab7-4fd8-a7fd-e8eaf712953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xi =  0.01\n",
      "validate-log RMSE = 0.10516573472862384\n",
      "extract-basic-stats RMSE = 0.03335112934388266\n",
      "extract-successes RMSE = 0.06560030166991808\n",
      "extract-success-stats RMSE = 0.055046243096374826\n",
      "validate-video RMSE = 0.22332641534815675\n",
      "cut-video RMSE = 0.15335394316190126\n",
      "merge-videos RMSE = 0.07055334119608661\n",
      "validate-video-face-recog RMSE = 0.18759368297553317\n",
      "transform-video RMSE = 0.16920015095993615\n",
      "detect-faces RMSE = 0.12713167112846488\n",
      "mark-faces RMSE = 0.049988356672170775\n"
     ]
    }
   ],
   "source": [
    "print('xi = ', bo_xi)\n",
    "\n",
    "for key, exhaustive_result in exhaustive_results.items():\n",
    "    bo_result = bo_results[key]\n",
    "    # exhaustive_largest_input_result = exhaustive_result.results[exhaustive_result.input_sizes[-1]]\n",
    "    # bo_largest_input_result = bo_result.results[bo_result.input_sizes[-1]]\n",
    "    # seaplot_exhaustive_bo_comparison(exhaustive_largest_input_result, bo_largest_input_result, key)\n",
    "    # rmse = normalized_rmse(exhaustive_largest_input_result, bo_largest_input_result)\n",
    "    rmse = mean_normalized_rmse(exhaustive_results=exhaustive_result, predicted_results=bo_result)\n",
    "    print(key, 'RMSE =', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf0544-2b35-41d3-a0c7-21b2d2478d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
